{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Doing Text Analysis with Azure Cognitive Services Text Analytics\n",
    "\n",
    "## Overview\n",
    "This notebook goes through the basics of writing a custom Cognitive Services Text Analytics application for analyzing long, open-form text. \n",
    "Specifically, we'll get key phrases and common entities (e.g. organizations) from each text input. \n",
    "\n",
    "The full SDK for Cognitive Services Text Analytics with Python can be found [here](https://azuresdkdocs.blob.core.windows.net/$web/python/azure-cognitiveservices-language-textanalytics/0.2.0/azure.cognitiveservices.language.textanalytics.html#azure.cognitiveservices.language.textanalytics.TextAnalyticsClient.key_phrases).\n",
    "\n",
    "### How to use this notebook\n",
    "This notebook contains all instructions and code needed to run text analytics on multiple text files. **For the code to function properly, you'll need to change two things:**\n",
    "\n",
    "1. Cognitive Services key, and\n",
    "2. Cognitive services endpoint.\n",
    "\n",
    "There are two ways to run the notebook code:\n",
    "\n",
    "1. Go through each section and click the green arrow <span style=\"color:green\">&#9655</span> on the top, left-hand side of each code block; or\n",
    "2. Run all cells by clicking the double-arrow icon at the very top of the notebook.\n",
    " \n",
    "### Let's get started!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## More information on Text Data\n",
    "This notebook takes a set of text stored in the same folder. These input text files can be different lengths, but distinct sentences or paragraphs should be separated by a new line.\n",
    "\n",
    "You may run this notebook on the sample text provided, which include text copied from Wikipedia articles. Each text file contains content from a single Wikipedia article, with the file name indicating the name of the article.\n",
    "\n",
    "Alternatively, you may upload your own text files into the **TextFiles** folder. How you organize the text into different files will depend on your particular needs. Here is an example scenario for handling open-form survey responses:\n",
    "\n",
    "1. Copy and paste all responses to a single question into a unique text file. \n",
    "    * Copying and pasting from Excel rows works great, as it will include a newline between each unique response.\n",
    "2. Save each text file in a subfolder called \"TextFiles\" (or update the filepath in the code block below where it says \"To Do\").\n",
    "\n",
    "### Loading the Data.\n",
    "Run the code below by clicking the <span style=\"color:green\">&#9655</span> to import and process the text files. Read through the comments for more details. \n",
    "\n",
    "**Troubleshooting**\n",
    "\n",
    "If you run into any issues as you run the code blocks, here are some troubleshooting tips that might be helpful:\n",
    "* The Cognitive Services Text Analytics client requires the following input format:\n",
    "    * Document size less than 5,120 characters (this is handled in the import data section)\n",
    "    * Input text should be in the following format: \n",
    "        ```text = {'id': Unique_Id, 'text': Input_Text}```\n",
    "    * You may input a list of (properly formatted) dictionaries. Note that the Text Analytics client will output attributes (e.g. key phrases) for each input dictionary.\n",
    "* An \"HttpOperationErrtext analytics client that the senot ntiment method did get a correctly formatted input. Some things to check:\n",
    "    * The document input contains unique ids fCheck the format of your input and make sure it matches the dictionary format above.nput\n",
    "    * Inputting \n",
    "* A \"list index out of range\" error indicates that the input text is too long for the Text Anal.\n",
    "* Run into other issues? Please let us know by opening an issue on the [GitHub repo](https://github.com/jenfoxbot/text-analytics-walkthrough).ytics client to handle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Note: your text files may have a different encoding than the one below\n",
    "enc = 'utf-8'\n",
    "\n",
    "# Read the text in the External_data/TextFilesfolder\n",
    "# TO DO: Update file path as necessary\n",
    "text_folder = os.path.join('TextFiles')\n",
    "\n",
    "# Create a collection of reviews with id (file name) and text (contents) properties\n",
    "texts = []\n",
    "for file_name in os.listdir(text_folder):\n",
    "    survey_text = open(os.path.join(text_folder, file_name), encoding=enc).read()\n",
    "    \n",
    "    # Counter to track number of times split is done \n",
    "    text_split_counter = 0\n",
    "    survey_text_list = []\n",
    "\n",
    "    if len(survey_text) > 5100:\n",
    "        while len(survey_text) > 5100:\n",
    "            for char in (0,5100):\n",
    "                # Get the first 5100 characters\n",
    "                text_holder = survey_text[0:5100]\n",
    "                # Go back to the last new line and save in a list. Save index of split\n",
    "                for item in range(len(text_holder[::-1])):\n",
    "                    #print(text_holder[item])\n",
    "                    if text_holder[item] == \"\\n\":\n",
    "                        # Store index of last newline\n",
    "                        index = item\n",
    "                        # Save list up to the last newline\n",
    "                        split_text = text_holder[0:index]\n",
    "            #Save split text\n",
    "            text = {\"id\": f\"{file_name}.{text_split_counter}\", \"text\": split_text}\n",
    "            texts.append(text)\n",
    "            # Update counter\n",
    "            text_split_counter += 1\n",
    "            # Remove split text from survey_text \n",
    "            survey_text = survey_text[index:len(survey_text)]\n",
    "        # Save split text and update text list        \n",
    "        text = {\"id\": f\"{file_name}.{text_split_counter}\", \"text\": survey_text}\n",
    "        texts.append(text)\n",
    "    \n",
    "    else:\n",
    "        text = {\"id\": file_name, \"text\": survey_text}\n",
    "        texts.append(text)\n",
    "\n",
    "for text_num in range(len(texts)):\n",
    "    # print the open-form text\n",
    "    print(texts[text_num]['id'])\n",
    "    # OPTIONAL: To see all text, uncomment the following 2 lines\n",
    "    #for i in range(number_of_lines):\n",
    "        #print('{}\\n{}\\n'.format(texts[text_num]['id'], texts[text_num]['text']))  "
   ]
  },
  {
   "source": [
    "## Get the Key and Endpoint for your Cognitive Services resource\n",
    "This notebook assumes you already have a Cognitive Services resource in your Azure subscription. (If not, follow the instructions in **Step 3** the [Read Me of this github repo](https://github.com/microsoft/text-analytics-walkthrough).)\n",
    "\n",
    "1. In another browser tab, open the Azure portal at https://portal.azure.com \n",
    "2. Select your cognitive services resource. On the **Overview** page, click on **\"Keys and endpoint\"** in the menu on the left-hand side (under Resource Management).\n",
    "3. Copy the **Key1** for your resource and paste it in the code below, replacing **YOUR_COG_KEY**.\n",
    "2. Copy the **endpoint** for your resource and and paste it in the code below, replacing **YOUR_COG_ENDPOINT**.\n",
    "3. Run the code in the cell below by clicking its green <span style=\"color:green\">&#9655</span> button.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Fill in your cognitive services key and endpoint\n",
    "cog_key = 'YOUR_COG_KEY'\n",
    "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
    "\n",
    "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
   ]
  },
  {
   "source": [
    "## Install Azure Cognitive Services Text Analytics SDK\n",
    "Run the code below to install the text analytics SDK onto the compute virtual machine."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-cognitiveservices-language-textanalytics"
   ]
  },
  {
   "source": [
    "## Import Cognitive Services libraries and create a Text Analytics client\n",
    "\n",
    "Run the following code to import the Cognitive Services Text Analytics library. We'll also create a client for the text analytics cognitive services resource, which takes in two inputs: your endpoint and key that we set above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azure.cognitiveservices.language.textanalytics import TextAnalyticsClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "# Get a client for your text analytics cognitive service resource\n",
    "text_analytics_client = TextAnalyticsClient(endpoint=cog_endpoint,\n",
    "                                            credentials=CognitiveServicesCredentials(cog_key))"
   ]
  },
  {
   "source": [
    "## Extract Key Phrases\n",
    "We're now ready to start using the text analytics service! First, let's get a list of key phrases from our set of texts. This helps give some indication of common themes and trends without us having to read all of the text.\n",
    "\n",
    "This code block outputs  key phrases for each set of texts and stores them in a text file. \n",
    "It also prints the file name, basic statistics for each text file (e.g. character count), and the number of key phrases.\n",
    "\n",
    "### More Info\n",
    "* The key phrases service processes each input as-a-whole. This means that key phrases are extracted based on the entire input text.\n",
    "* The number of returned key phrases is proportional to the size of the input text.\n",
    "* The SDK documentation for this service can be found [here](https://azuresdkdocs.blob.core.windows.net/$web/python/azure-cognitiveservices-language-textanalytics/0.2.0/azure.cognitiveservices.language.textanalytics.html#azure.cognitiveservices.language.textanalytics.TextAnalyticsClient.key_phrases)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in text file with responses\n",
    "key_phrase_analysis = text_analytics_client.key_phrases(documents=texts, show_stats=True)\n",
    "\n",
    "# print and store key phrases for each text file\n",
    "for text_num in range(len(texts)):\n",
    "    # Open file with file header name given by \n",
    "    file_header = texts[text_num]['id']\n",
    "    \n",
    "    # print the review id\n",
    "    print(texts[text_num]['id'])\n",
    "\n",
    "    # Print stats for the file\n",
    "    stats = key_phrase_analysis.documents[text_num].statistics\n",
    "    print('\\nStatistics: ')\n",
    "    print(stats)\n",
    "\n",
    "    # Get the key phrases in this review\n",
    "    print('\\nKey Phrases:')\n",
    "    key_phrases = key_phrase_analysis.documents[text_num].key_phrases\n",
    "    print('\\nNumber of Key Phrases')\n",
    "    print(len(key_phrases))\n",
    "\n",
    "    # Print each key phrase\n",
    "    for key_phrase in key_phrases:\n",
    "        print('\\t', key_phrase)\n",
    "    print('\\n')"
   ]
  },
  {
   "source": [
    "## Extract Known Entities\n",
    "\n",
    "Next, let's look for common entities in our text responses. *Entities* are things that reference some commonly understood type of item. \n",
    "For example, a location, a person, or an organization. The following code block pulls out \"Organization\" and \"Other\" entities. \n",
    "\n",
    "A full list of supported entities can be found in the [Azure Cognitive Services Python SDK documentation here](https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/named-entity-types?tabs=general)\n",
    "\n",
    "### More Info\n",
    "Some entities are sufficiently well-known to have an associated Wikipedia page. The Text Analytics service can also return the URL for that page. \n",
    "To enable this, uncomment the line below the code comment that says \"Get Wikipedia link\"."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the client and reviews you created previously to get named entities\n",
    "entity_analysis = text_analytics_client.entities(documents=texts)\n",
    "\n",
    "# Print the results for each review\n",
    "for review_num in range(len(texts)):\n",
    "    print(texts[review_num]['id'])\n",
    "    # Get the named entitites in this review\n",
    "    entities = entity_analysis.documents[review_num].entities\n",
    "\n",
    "    for entity in entities:\n",
    "        # Only get Organization and Other entitites\n",
    "        # Add or remove entities to this list according to needs/interests\n",
    "        if entity.type in ['Organization','Other']:\n",
    "            # Get Wikipedia link\n",
    "            #link = '(' + entity.wikipedia_url + ')' if entity.wikipedia_id is not None else ''\n",
    "            print(' - {}: {}'.format(entity.type, entity.name))"
   ]
  },
  {
   "source": [
    "## Going Further\n",
    "\n",
    "Congratulations! You did machine learning on text to get some (hopefully) helpful insights! There are other services available to you, including sentiment analysis and language detection. For more information on those services, check out the Microsoft Docs [Python Cognitive Services Text Analytics SDK](https://docs.microsoft.com/en-us/python/api/azure-cognitiveservices-language-textanalytics/azure.cognitiveservices.language.textanalytics.text_analytics_client.textanalyticsclient?view=azure-python) or \n",
    "\n",
    "You can also use Azure Cognitive Services for all sorts of other things like Computer Vision, Anomaly Detection, and Speech Recognition! [Here's a handy overview](https://docs.microsoft.com/en-us/python/api/overview/azure/cognitive-services?view=azure-python) that covers what else you can do in Python.\n",
    "\n",
    "Questions? Requests? Let us know! Open a Pull Request on our repo or send us an e-mail: AskAMaker@microsoft.com\n",
    "\n",
    "Thanks for reading!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}