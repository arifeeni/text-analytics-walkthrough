{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('.venv')",
   "metadata": {
    "interpreter": {
     "hash": "f1d914c214990b2bee8c896321f7049c25f62f215b6ebe15a146ac9059173220"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Doing Text Analysis with Azure Cognitive Services Text Analytics\n",
    "\n",
    "## Overview\n",
    "This notebook goes through the basics of writing a custom Cognitive Services Text Analytics application for analyzing long, open-form text. \n",
    "Specifically, we'll get key phrases and common entities (e.g. organizations) from each text input. \n",
    "\n",
    "Documentation for how to use Cognitive Services Text Analytics with Python can be found [here (SDK V3)](https://docs.microsoft.com/en-us/python/api/azure-ai-textanalytics/azure.ai.textanalytics.textanalyticsclient?view=azure-python).\n",
    "\n",
    "### How to use this notebook\n",
    "This notebook contains all instructions and code needed to run text analytics on multiple text files. **For the code to function properly, you'll need to add two things in an untracked file:**\n",
    "\n",
    "1. Text Analytics key, and\n",
    "2. Text Analytics endpoint.\n",
    "\n",
    "Remember to treat these Azure credentials like passwords: keep them private and secure! \n",
    "\n",
    "**There are two ways to run the notebook code:**\n",
    "\n",
    "1. Go through each section and click the green arrow <span style=\"color:green\">&#9655</span> on the top, left-hand side of each code block; or\n",
    "2. Run all cells by clicking the double-arrow icon at the very top of the notebook.\n",
    " \n",
    "\n",
    "### Let's get started!\n",
    "Read through the section below to learn more about Text Analytics. Then run the code block below to import and process the sample text files stored in the *TextFiles* folder.\n",
    "\n",
    "To process your own text, replace the sample text files in the *TextFiles* folder with your own .txt files."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## More information on Text Data\n",
    "This notebook takes a set of text stored in the **TextFiles** folder. These input text files can be different lengths, but distinct sentences or paragraphs should be separated by a new line.\n",
    "\n",
    "You may run this notebook on the sample text provided, which include text copied from Wikipedia articles. Each text file contains content from a single Wikipedia article, with the file name indicating the name of the article.\n",
    "\n",
    "Alternatively, you may upload your own text files into the **TextFiles** folder. How you organize the text into different files will depend on your particular needs. Here is an example scenario for handling open-form survey responses:\n",
    "\n",
    "1. Copy and paste all responses to a single question into a unique text file. \n",
    "    * Copying and pasting from Excel rows works great, as it will include a newline between each unique response.\n",
    "2. Save each text file in a subfolder called \"TextFiles\" (or update the filepath in the code block below where it says \"To Do\").\n",
    "\n",
    "**Troubleshooting**\n",
    "\n",
    "If you run into any issues as you run the code blocks, here are some troubleshooting tips that might be helpful:\n",
    "* The Cognitive Services Text Analytics Python SDK V3 requires the following input formats:\n",
    "    * Raw Text Input\n",
    "    * Encoding: UTF-8 or UTF-16\n",
    "    * Document size less than 5,120 characters (this is handled in the import data section)\n",
    "    * Some methods (e.g. entity recognition) require a batch size of 5 or less.\n",
    "    * Input text should be in one of the following formats:\n",
    "        * a list of strings: \n",
    "        \n",
    "            ```list[str]```\n",
    "        * a list of text documents: \n",
    "        \n",
    "            ```list[TextDocumentInput]```\n",
    "        * or a list of dictionary representations with at least two string elements, ID and text:\n",
    "\n",
    "             ```list[dict[str, str]]``` \n",
    "             \n",
    "             E.g. ```texts = [{'id': Unique_Id1, 'text': Input_Text1}, {'id': Unique_ID2, 'text': Input_Text2}, ...]```\n",
    "    * If you wish to specify the ID and language on a per-item basis you must use as input a list of text documents or a list of dict representations. \n",
    "    * [More info on data limits here](https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/overview#data-limits)\n",
    "* An \"HttpOperationErrText\", \"HttpResponseError\" or similar error indicates that the Text Analytics method did not get a correctly formatted input. Some things to check:\n",
    "    * Batch size limits.\n",
    "    * Iterating through multiple text files such that the method inputs fit one of the three options above.\n",
    "    * Any list of dictionary inputs contains unique ids for each dictionary. \n",
    "* A \"list index out of range\" error indicates that the input text is too long for the Text Analytics client to handle.\n",
    "* Run into other issues? Please let us know by opening an issue on the [GitHub repo](https://github.com/jenfoxbot/text-analytics-walkthrough)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code imports text files stored in 'TextFiles' folder and splits them \n",
    "# into dictionaries of fewer than 5,100 characters at the last new line.\n",
    "# The final dictionary names are printed to the screen.\n",
    "\n",
    "import os\n",
    "# Note: your text files may have a different encoding than the one below\n",
    "enc = 'utf-8'\n",
    "\n",
    "# Import the text files in the TextFiles folder\n",
    "# TO DO: Update file path as necessary\n",
    "text_folder = os.path.join('TextFiles')\n",
    "\n",
    "# Create a collection of texts with id (file name) and text (contents) properties\n",
    "texts = []\n",
    "for file_name in os.listdir(text_folder):\n",
    "    survey_text = open(os.path.join(text_folder, file_name), encoding=enc).read()\n",
    "    \n",
    "    # Counter to track number of times split is done \n",
    "    text_split_counter = 0\n",
    "    survey_text_list = []\n",
    "\n",
    "    if len(survey_text) > 5100:\n",
    "        while len(survey_text) > 5100:\n",
    "            for char in (0,5100):\n",
    "                # Get the first 5100 characters\n",
    "                text_holder = survey_text[0:5100]\n",
    "                # Go back to the last new line and save in a list. Save index of split\n",
    "                for item in range(len(text_holder[::-1])):\n",
    "                    #print(text_holder[item])\n",
    "                    if text_holder[item] == \"\\n\":\n",
    "                        # Store index of last newline\n",
    "                        index = item\n",
    "                        # Save list up to the last newline\n",
    "                        split_text = text_holder[0:index]\n",
    "            #Save split text\n",
    "            text = {\"id\": f\"{file_name}.{text_split_counter}\", \"text\": split_text}\n",
    "            texts.append(text)\n",
    "            # Update counter\n",
    "            text_split_counter += 1\n",
    "            # Remove split text from survey_text \n",
    "            survey_text = survey_text[index:len(survey_text)]\n",
    "        # Save split text and update text list        \n",
    "        text = {\"id\": f\"{file_name}.{text_split_counter}\", \"text\": survey_text}\n",
    "        texts.append(text)\n",
    "    \n",
    "    else:\n",
    "        text = {\"id\": file_name, \"text\": survey_text}\n",
    "        texts.append(text)\n",
    "\n",
    "for text_num in range(len(texts)):\n",
    "    # print the open-form text\n",
    "    print(texts[text_num]['id'])\n",
    "    # OPTIONAL: To see all text, uncomment the following 2 lines\n",
    "    #for i in range(number_of_lines):\n",
    "        #print('{}\\n{}\\n'.format(texts[text_num]['id'], texts[text_num]['text']))  "
   ]
  },
  {
   "source": [
    "## Get the Key and Endpoint for your Text Analytics resource\n",
    "This notebook assumes you already have a Text Analytics resource in your Azure subscription. (If not, follow the instructions in **Step 1** the [Read Me of this github repo](https://github.com/microsoft/text-analytics-walkthrough).)\n",
    "\n",
    "To use Azure Text Analytics, you'll need a key, similar to password, and an endpoint, which accesses the Text Analytics resource you created. It's important to keep both of these private and secure! \n",
    "\n",
    "### A. Create a private file to store your Azure key and endpoint.\n",
    "The code block below installs a Python library called *dotenv*, which allows you to read environment variables from a file. You can store your Azure key and endpoint in this (untracked) file, so if you fork this project your Azure credentials remain private.\n",
    "\n",
    "Run the code below to install the library and create the *.env* file where you'll store your key and endpoint.\n",
    "\n",
    "*Note: If you're on a Linux or Unix machine, use the appropriate command line calls.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOWS: Use the following commands\n",
    "!pip install python-dotenv\n",
    "\n",
    "!echo YOUR_COG_KEY= > .env\n",
    "!echo YOUR_COG_ENDPOINT= >> .env"
   ]
  },
  {
   "source": [
    "### B. Get your Azure key and endpoint and store them in the *.env* file.\n",
    "\n",
    "1. In VS Code, open the *.env* file you just created. You should see two (blank) variables: *YOUR_COG_KEY* and *YOUR_COG_ENDPOINT*. \n",
    "1. In a browser window, open the [Azure portal](https://portal.azure.com).\n",
    "2. Select your Text Analytics resource. On the **Overview** page, click on **\"Keys and endpoint\"** in the menu on the left-hand side (under Resource Management).\n",
    "1. Copy the **Key1** for your resource and paste it into the *.env* file for **YOUR_COG_KEY** after the equals sign and **between quotes**, without any spacing, like so:\n",
    "    ```YOUR_COG_KEY='COG_KEY_HERE'```\n",
    "2. Copy the **endpoint** for your resource and and paste into the *.env* file for **YOUR_COG_ENDPOINT**:\n",
    "    ```YOUR_COG_ENDPOINT='COG_ENDPOINT_HERE'```\n",
    "3. Run the code in the cell to load the variables from the *.env* file into this notebook environment."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dotenv library functions and load in Azure credentials from .env file, \n",
    "# Print if key and endpoint loaded successfully.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('./.env', override=True)\n",
    "\n",
    "cog_key = os.getenv('YOUR_COG_KEY')\n",
    "cog_endpoint = os.getenv('YOUR_COG_ENDPOINT')\n",
    "\n",
    "if cog_key and cog_endpoint:\n",
    "    print('Ready to use cognitive services!')\n",
    "else: print('Unable to load key and/or endpoint.')"
   ]
  },
  {
   "source": [
    "## Install Azure Cognitive Services Text Analytics SDK\n",
    "Run the code below to install the text analytics SDK onto your local environment."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-ai-textanalytics --pre"
   ]
  },
  {
   "source": [
    "## Import Cognitive Services libraries and create a Text Analytics client\n",
    "\n",
    "Run the following code to import the Cognitive Services Text Analytics library. We'll also create a client for the text analytics cognitive services resource, which takes in two inputs: your key and endpoint that we set above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(cog_key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=cog_endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()"
   ]
  },
  {
   "source": [
    "## Extract Key Phrases\n",
    "We're now ready to start using the text analytics service! First, let's get a list of key phrases from our set of texts. This helps give some indication of common themes and talking points without us having to read all of the text.\n",
    "\n",
    "When you run the code block below, it outputs key phrases for each set of texts. \n",
    "It also prints the file name, basic statistics for each text file (e.g. character count), and the number of key phrases.\n",
    "\n",
    "### More Info\n",
    "* The key phrases service processes each input as-a-whole. This means that key phrases are extracted based on the entire input text.\n",
    "* The number of returned key phrases is proportional to the size of the input text.\n",
    "* The SDK documentation for this service can be found [here](https://docs.microsoft.com/en-us/python/api/azure-cognitiveservices-language-textanalytics/azure.cognitiveservices.language.textanalytics.textanalyticsclient?view=azure-python#key-phrases-show-stats-none--documents-none--custom-headers-none--raw-false----operation-config-)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in text files and do key phrase extraction on each file\n",
    "key_phrase_analysis = client.extract_key_phrases(documents=texts, show_stats=True)\n",
    "\n",
    "#For each text file, get and print key phrases and document statistics\n",
    "for doc in key_phrase_analysis:\n",
    "    if not doc.is_error:\n",
    "        # Print the document title\n",
    "        print(doc['id'])\n",
    "        # Print stats for each file\n",
    "        stats = doc.statistics\n",
    "        print('\\nStatistics: {}'.format(stats))\n",
    "        \n",
    "        # Get the key phrases in this review\n",
    "        key_phrases = doc.key_phrases\n",
    "        print('\\nNumber of Key Phrases: {}'.format(len(key_phrases)))\n",
    "        # Print each key phrases on a new line\n",
    "        for key_phrase in key_phrases:\n",
    "            print('\\t', key_phrase)\n",
    "        print('\\n')\n",
    "\n",
    "    if doc.is_error:\n",
    "        print(doc.id, doc.error)"
   ]
  },
  {
   "source": [
    "## Extract Known Entities\n",
    "\n",
    "Next, let's look for common entities in our text responses. *Entities* are things that reference some commonly understood type of item. \n",
    "For example, a location, a person, or an organization. The following code block pulls out \"Organization\", \"Person\", \"Location, and \"Other\" entities. \n",
    "\n",
    "A full list of supported entities can be found [here](https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/named-entity-types?tabs=general).\n",
    "\n",
    "### More Info\n",
    "Some entities are sufficiently well-known to have an associated Wikipedia page. The Text Analytics service can also return the URL for that page using the \"recognize_linked_entities\" method. For more information, [check out the Python SDK method documentation here](https://docs.microsoft.com/en-us/python/api/azure-ai-textanalytics/azure.ai.textanalytics.textanalyticsclient?view=azure-python#recognize-entities-documents----kwargs-)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the client and reviews you created previously to get named entities\n",
    "# Set batch size to be 5 (max input for recognize_entities)\n",
    "batch_size = 5\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    # Create subset of dictionaries\n",
    "    batch = texts[i:i+batch_size] \n",
    "    #Run entity analysis on sets of 5 or fewer inputs \n",
    "    entity_analysis = client.recognize_entities(batch)\n",
    "    \n",
    "    # Create a list of the entity analysis results\n",
    "    docs = [doc for doc in entity_analysis if not doc.is_error]\n",
    "\n",
    "    # Iterate through texts and print entity analysis results\n",
    "    for idx, doc in enumerate(docs):\n",
    "        # Print text ID \n",
    "        print(\"\\n\" + texts[idx]['id'])\n",
    "        for entity in doc.entities:\n",
    "            # Only get Organization and Other entitites\n",
    "            # TO DO: Add or remove entities to this list according to needs/interests\n",
    "            if entity.category in ['Organization','Person', 'Location', 'Other']:\n",
    "                print(' - {}: {} (Confidence: {})'.format(entity.category, entity.text, entity.confidence_score))"
   ]
  },
  {
   "source": [
    "## Going Further\n",
    "\n",
    "Congratulations! You did machine learning on text to get some (hopefully) helpful insights!  \n",
    "\n",
    "There are other services available to you, including sentiment analysis and language detection. For more information on those services, check out the Microsoft Docs [Python Cognitive Services Text Analytics SDK](https://docs.microsoft.com/en-us/python/api/azure-ai-textanalytics/azure.ai.textanalytics.textanalyticsclient?view=azure-python). Fun fact: you can also do [sentiment analysis in Excel](https://www.mrexcel.com/excel-tips/sentiment-analysis/)!\n",
    "\n",
    "You can also use Azure Cognitive Services for all sorts of other things like Computer Vision, Anomaly Detection, and Speech Recognition! [Here's a handy overview](https://docs.microsoft.com/en-us/python/api/overview/azure/cognitive-services?view=azure-python) that covers what else you can do in Python.\n",
    "\n",
    "Questions? Requests? Let us know! Open a Pull Request on our repo or send us an e-mail: AskAMaker@microsoft.com\n",
    "\n",
    "Thanks for reading!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}